{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In the production of electrical drives, a high product quality is needed. As the industry of electric drive production is confronted by trends such as electric mobility and continuing industrial automation, efficient and flexible processes are needed more than ever. With current quality monitoring technology, accurate quality checking is not feasible. \n",
    "\n",
    "Electrical motors mainly consist of the rotor, the stator and the surrounding housing. The production process can be separated into multiple sub-processes, which can be seen below. The exact sequence of these steps however depends on the motor type. First, the individual components are manufactured and assembled into subassemblies such as the rotor and the stator. Finally, all components (the housing, the stator, the rotor as well as bearings and end shields) are assembled and the motor is checked in an end-of-line (EOL) test.\n",
    "\n",
    "This final assembly is of great importance, as all parts need to be assembled in the correct way, to ensure smooth operation. Therefore, a quality monitoring system is needed, raising alarm if assembly errors are detected. However, especially in lot-size one production, traditional computer vision systems might reach their limits and cannot be used anymore. \n",
    "\n",
    "Thus, in this lab we will build a smart quality monitoring system for the electric drives production. An already existing visual sensor captures images of the electric motor after assembly. These images show the part from the top, as well from the side perspective. It is now the target to decide whether the motor is fully assembled, or whether one of multiple defects is present. There is data from three different defects available: *missing cover*, *missing screw* and *not screwed*. Examples of these defects can be seen below. To achieve this, we will investigate two different machine learning models: *Support Vector Machines* (SVM) and *Convolutional Neural Networks* (CNN).\n",
    "\n",
    "Further background information can be found in this paper: [Mayr et al., Machine Learning in Electric Motor Production - Potentials, Challenges and Exemplary Applications](https://ieeexplore.ieee.org/document/9011861)\n",
    "\n",
    "![Introduction](./img/Intro_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outline\n",
    "This lab is structured into two main parts. In the **first part**, a subset of the problem will be analyzed step-by-step. Here, only images from the top view are used and only two of the three defects, the defects *missing cover* and *missing screw* are considered. Your task will be to follow along, fill out missing gaps, and answer problems throughout the notebook. \n",
    "\n",
    "In the **second part**, you are tasked to expand the quality monitoring system to also detect the defect *not screwed*. Therefore, it might be helpful to also consider images showing the parts in their side perspective. For this part, you are free to choose any of the tools and methods introduced in the first part, and you can expand as you wish!\n",
    "\n",
    "#### Deliverables\n",
    "For completing this exercise successfully, you need to deliver certain results. Throughout the notebook you will find **questions** you need to answer, and coding **tasks** where you need to modify existing code or fill in blanks. Answers to the questions need to be added in the prepared *Your answer here* markdown fields. Coding tasks can be solved by modifying or inserting code in the cells below the task. If needed, you can add extra cells to the notebook, as long as the order of the existing cells remains unchanged. Once you are finished with the lab, you can submit it through the procedure described in the forum. Once the labs are submitted, you will receive feedback regarding the questions. Thus, the **Feedback** boxes need to be left empty\n",
    "\n",
    "Example:\n",
    "\n",
    ">**Question:** What do I do if I am stuck solving this lab?\n",
    "\n",
    ">**Your answer:** <span style=\"color:green\">Have a look at the forum, maybe some of your peers already experienced similiar issues. Otherwise start a new discussion to get help!</span>\n",
    "\n",
    ">**Feedback:** <span style=\"color:orange\">This is a great approach! Besides asking in the forum I'd also suggest asking your tutor.</span>\n",
    "\n",
    "#### Ressources\n",
    "If you are having issues while completing this lab, feel free to post your questions into the forum. Your peers as well as our teaching advisors will screen the forum regularly and answer open questions. This way, the information is available for fellow students encountering the same issues.\n",
    "\n",
    "#### Useful links\n",
    "\n",
    "- Forum on StudON: https://www.studon.fau.de/studon/goto.php?target=frm_3012142\n",
    "- Technical instructions for setting up your own coding environment: [How to set up your lab](https://www.studon.fau.de/studon/ilias.php?ref_id=3012153&cmd=view&cmdClass=ilobjcontentpagegui&cmdNode=yl:mv&baseClass=ilRepositoryGUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in the previous exercises, we'll import commonly used libraries right at the start\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "# The check.py script contains the quality gates you can use for selftesting throughout the lab\n",
    "from scripts.check import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Part one\n",
    "To achieve the solution mentioned above, we will execute the following steps in this lab:\n",
    "\n",
    "1. First, we will code the necessary functions for loading and preprocessing of the data. We will also set up some methods that help us displaying our progress throughout the exercise\n",
    "2. Second, we will do a short analysis of the existing dataset\n",
    "3. Afterwards, we will start building our first image classification model using SVMs\n",
    "4. Once we are familiar and comfortable with SVMs, we will switch to neural networks and try out CNNs\n",
    "5. Finally, we will introduce data augmentation for improvement of our prediction results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Data preprocessing\n",
    "The data should be located in the folder called *data* on the same level as this script. Within this folder, two subfolders can be found:\n",
    "- The folder **top** contains the top view of each motor\n",
    "- The folder **side** contains the side view of each motor\n",
    "\n",
    "Each motor is uniquely identified by its filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading one image in top view\n",
    "path = \"./data/top/L1_C_3.JPG\"\n",
    "img = cv2.imread(path)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the snippet above, we are able to load the image from the file into a numpy array, while getting its label from the folder path the image is in.\n",
    "To check the type of a python object, you can use the command `type(img)`. It should return *numpy.ndarray*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to plot the image. This can be achieved by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(path.split('/')[-1]) # Set the filename as image title\n",
    "plt.imshow(img) # Display the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, open cv assumes the images are encoded in blue, green and red. However, the actual order of the color channels is blue, red and green. Thus, the channels need to be converted using `cv2.COLOR_BGR2RGB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert image from bgr to rgb\n",
    "plt.title(path.split('/')[-1]) # Set the filename as image title\n",
    "plt.imshow(img) # Display the image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for loading multiple images\n",
    "Now it's your turn. For the further analysis, we need to load all the available images from the given data folder *folder*. Besides the image, we need to also find the class of the respective image. The information of the class is encoded in the title of each image. You can use the helper function `get_label_from_name(path)` to parse the filename to the class.\n",
    "\n",
    ">**Task:** Please complete the following function **load_features_labels(folder)**. The function should read the image for a given file, and return two lists:\n",
    ">- *features* containing all the images as numpy arrays\n",
    ">- *labels* containing the classes of all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def get_label_from_name(path):\n",
    "    if \"_C_\" in path:\n",
    "        return \"Complete\"\n",
    "    if \"_MC_\" in path:\n",
    "        return \"Missing cover\"\n",
    "    if \"_MS_\" in path:\n",
    "        return \"Missing screw\"\n",
    "    if \"_NS_\" in path:\n",
    "        return \"Not screwed\"\n",
    "    return \"n/a\" # TODO: Raise error\n",
    "\n",
    "def load_features_labels(folder, size = (64,32), flatten = True, color = False, identifiers=['NS', 'MS', 'MC', 'C']):\n",
    "    features, labels = [], [] # Empty lists for storing the features and labels\n",
    "    # Iterate over all imagefiles in the given folder\n",
    "    for file in glob.glob(folder + \"/*.JPG\"):\n",
    "        if any(identifier in file for identifier in identifiers):\n",
    "            #############################\n",
    "            # Please add your code here #\n",
    "            #############################\n",
    "\n",
    "\n",
    "    \n",
    "    return features, labels # Return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything works as expected, the function should load 117 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = load_features_labels(\"./data/top\")\n",
    "print(\"Number of features:\", len(features))\n",
    "print(\"Number of labels:\", len(labels))\n",
    "\n",
    "# Check data import\n",
    "quality_gate_111(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image preprocessing\n",
    "Before analyzing the images using machine learning, they need to be preprocessed. We will do preprocessing regarding three aspects:\n",
    "- **Image size**: As the raw images are available in rather high resolution, it might be beneficial to reduce the image resolution. Opencv provides the function `resize()` which works great for that purpose\n",
    "- **Image color**: In many use cases, the benefit of considering color information might not outway the increased complexity, thus it might be handy to convert the rgb image to bw. This can easily be done using the `cvtColor` function from opencv.\n",
    "- **Image shape**: Only some algorithms are capable of analyzing the 2.5D structure of image data. For the remaining algorithms, which expect the data to be 1D vector, the image data needs to be flattened from 2.5D to 1D. This can be done using the numpy `reshape` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(img, size = (64,32), flatten = True, color = False):\n",
    "    img = cv2.resize(img, size)\n",
    "    if not color:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    if flatten:\n",
    "        img = img.reshape(-1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task:** Please update your `load_features_labels(...)` function from above to do image wise data preprocessing using the function `image_preprocessing(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_labels(folder, size = (64,32), flatten = True, color = False, identifiers=['NS', 'MS', 'MC', 'C']):\n",
    "    features, labels = [], [] # Empty lists for storing the features and labels\n",
    "    # Iterate over all imagefiles in the given folder\n",
    "    for file in glob.glob(folder + \"/*.JPG\"):\n",
    "        if any(identifier in file for identifier in identifiers):\n",
    "            #############################\n",
    "            # Please add your code here #\n",
    "            #############################\n",
    "\n",
    "    \n",
    "    return features, labels # Return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Quality gate #\n",
    "################\n",
    "\n",
    "features, labels = load_features_labels(\"./data/top\", size=(8, 8), flatten=True, color=False)\n",
    "quality_gate_112(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: First data analysis\n",
    "Before diving into machine learning, we'll have a look at the data. With the snippet below you can visualize a sample of the image data available in this lab. It can be observed that the class *missing cover* is rather distinct to the remaining classes, as the large black plastic cover is missing, exposing the copper wires. The defect *missing screw* is definitely harder to spot as the screws are rather small objects and the color difference between the screw and the empty hole is rather subtle. Finally, the defect *not screwed* can only be seen as some of the screws are not in the shade of the respective hole, thus indicating they are not screwed in all the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(3, 4), axes_pad=(0.1, 0.3))\n",
    "\n",
    "features, labels = load_features_labels(\"./data/top\", size=(1024, 1024), flatten=False, color=True)\n",
    "classes = ['Complete', 'Missing cover', 'Missing screw', 'Not screwed']\n",
    "for i, ax in enumerate(grid):\n",
    "    selectedClass = classes[i%4] # Select class\n",
    "    images = np.array(features)[np.array(labels)==selectedClass] # Preselect images based on class\n",
    "    image = images[i//4] # Select image \n",
    "    ax.imshow(image) # Plot image\n",
    "    ax.set_title(selectedClass) # Assign class as image title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's investigate the distribution of the available images among the classes.\n",
    "\n",
    ">**Task:** Please create a plot showing the distribution of the different classes and discuss the distribution in the field below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Please add your code here #\n",
    "#############################\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question:** Please discuss the class distribution. Which issues and challenges might appear during model training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Your answer:** <span style=\"color:green\">TBD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Feedback:** <span style=\"color:orange\">...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Image classification using Support Vector Machines\n",
    "In this section, we'll use Support Vector Machines (SVM) to try classifying the image dataset. For SVMs, it is necessary to have the data formatted as 1D vector. Also, as mentioned in the description we are only going to consider the three classes *complete*, *missing cover*, and *missing screws*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = load_features_labels(\"./data/top\", size=(16,16), color=True, flatten=True, identifiers=['MC', 'MS', 'C'])\n",
    "features = np.asarray(features)\n",
    "labels = np.asarray(labels)\n",
    "print(\"Shape feature vector:\", features.shape)\n",
    "print(\"Shape label vector:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we still load our 117 images, but the pixel values are now simply reshaped to 1D.\n",
    "Next, we need to separate our data into training and testing datasets. This can be achieved using the `train_test_split()` function from sklearn. You can find the documentation here: [Link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). \n",
    "\n",
    ">**Task:** Fill in the following code so that 70% of the data is used for training, and the remaining 30% for testing. Also, the datasets should be stratified by the label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "######################################\n",
    "# Please complete the following line #\n",
    "######################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(\"\"\"Your code goes here\"\"\", random_state=42)\n",
    "\n",
    "################\n",
    "# Quality gate #\n",
    "################\n",
    "\n",
    "quality_gate_13(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "clf = SVC(kernel=\"rbf\", gamma=0.01, C=0.0003) # Initialize the SVM\n",
    "clf.fit(X_train, y_train, sample_weight=compute_sample_weight('balanced', y_train)) # Train the SVM\n",
    "print(\"Score:\", clf.score(X_test, y_test)) # Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_test, clf.predict(X_test))\n",
    "ax=sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4: Image classification with artificial neural networks\n",
    "\n",
    "In this section, we will train our first artificial neural network (ANN) for image classification.\n",
    "First, we will have a look at normal ANNs. These consist of multiple dense layers which can analyze one-dimensional feature vectors. Thus, we need to reshape our 2.5-dimensional image data to 1D using the `flatten` option we integrated into our preprocessing function.\n",
    "\n",
    "### a) Image classification using fully connected ANNs\n",
    "Again, we need to load the data using the `flatten=True` flag to convert the 2.5D data to 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = load_features_labels(\"./data/top\", size=(128,128), color=True, flatten=True, identifiers=['MC', 'MS', 'C'])\n",
    "features = np.asarray(features)\n",
    "labels = np.asarray(labels)\n",
    "print(\"Shape feature vector:\", features.shape)\n",
    "print(\"Shape label vector:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task:** Fill in the following code so that 70% of the data is used for training, and the remaining 30% for testing. Also the datasets should be stratified by the label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Please complete the following line #\n",
    "######################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(\"\"\"Your code goes here\"\"\", random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels need to be *one hot encoded*. In one hot encoding, categorical values are transformed into a binary representation.\n",
    "\n",
    "![OneHotEncoding](./img/OneHotEncoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sklearn preprocessing library contains a variety of useful data preprocessing tools such as one hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Display the first label before encoding\n",
    "print(\"Label of first sample before OneHot encoding:\", y_train[0])\n",
    "# Create the encoder object\n",
    "enc = OneHotEncoder(sparse=False) # Generate Encoder\n",
    "# With the fit_transform function, the encoder is fitted to the existing labels and transforms the dataset into its binary representation\n",
    "y_train = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "# Display the first label after encoding\n",
    "print(\"Label of first sample after OneHot encoding:\", y_train[0])\n",
    "# Data preprocessing should always be fitted on the training dataset, but applied to both, the training and the testing dataset. Thus the fit_transform function is only applied to the training dataset, while the the test dataset is transformed using the transform function and the fitted preprocessing module\n",
    "y_test = enc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a simple ANN with an input layer, 3 hidden layer and one output layer.\n",
    "In this lab we use the **keras** library to model the neural network. \n",
    "\n",
    "A simple ANN with multiple sequential layers can be created using the `Sequential()` model. Afterwards, various layers can be added to the model through the command `model.add(LAYER)` with *LAYER* defining the layer to be added. In the first layer, the shape of the input needs to be specified using the parameter `input_shape`. This is only necessary in the first, but not in consecutive layers.\n",
    "\n",
    "Please have a look at the keras documentation regarding the [sequential model](https://keras.io/api/models/sequential/) and the [various layers](https://keras.io/api/layers/). For now, especially the core layers [Dense](https://keras.io/api/layers/core_layers/dense/) and [Activation](https://keras.io/api/layers/core_layers/activation/) are of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape = X_train[0].shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(y_train[0].shape[0]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is created, `model.summary()` displays the architecture of the model. You can see that the created model consists of three *dense* layers, each with an *activation* function. Also, the parameter for each layer are visible. Depending on the selected image size during preprocessing, the input vector might be rather large, thus the high number of parameters in the first dense layer. \n",
    "\n",
    "Next, the model needs to be compiled using a `loss` function and an `optimizer`. The loss function defines how the loss is computed during model training, while the optimizer defines how the weights need to be adjusted during backpropagation. You can find more information regarding the available losses [here](https://keras.io/api/losses/) and regarding the optimizers [here](https://keras.io/api/optimizers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the model can be trained using the datasets defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs = 20, batch_size = 8, validation_split=0.2, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following function to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_test, y_test, model):\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    res = np.zeros_like(y_pred)\n",
    "    for i in range(len(np.argmax(y_pred, axis=1))):\n",
    "        res[i, np.argmax(y_pred,axis=1)[i]]=1\n",
    "    y_pred = res\n",
    "    cm = confusion_matrix(enc.inverse_transform(y_test), enc.inverse_transform(y_pred))\n",
    "    ax=sns.heatmap(cm, annot=True)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Truth')\n",
    "    print(classification_report(enc.inverse_transform(y_test), enc.inverse_transform(y_pred), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question:** What behavior did you observe while training the model? How can the results be explained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Your answer:** <span style=\"color:green\">TBD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Feedback:** <span style=\"color:orange\">...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Image classification using CNNs\n",
    "\n",
    "In this section, we are going to explore the usage of CNNs for the given task.\n",
    "\n",
    "![Architecture CNN](./img/cnn_structure.png)\n",
    "\n",
    "First, the data is loaded from file. As CNNs are capable and even excel on analyzing the multiple dimensional aspects of images, the images do not need to be reshaped in a one-dimensional vector. Thus, we have to set the flag `flatten` to `False`. You can see, that the shape of the loaded images is now a four-dimensional array with `(number of samples, width image, height image, color channels image)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = load_features_labels(\"./data/top\", size=(512,512), color=True, flatten=False, identifiers=['MC', 'MS', 'C'])\n",
    "features = np.array(features) # Datatype conversion of feature vector from list to array\n",
    "labels = np.array(labels) # Datatype conversion of label vector from list to array\n",
    "print(\"Shape feature vector:\", features.shape)\n",
    "print(\"Shape label vector:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task:** Fill in the following code so that 70% of the data is used for training, and the remaining 30% for testing. Also, the datasets should be stratified by the label vector. Furthermore, add OneHot Encoding for the labels as seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Please complete the following lines #\n",
    "#######################################\n",
    "\n",
    "def split_data(features, labels):\n",
    "    return train_test_split(\"\"\"Your code goes here\"\"\")\n",
    "\n",
    "def encode_data(y_train, y_test):\n",
    "    \"\"\"Your code goes here\"\"\"\n",
    "    return y_train, y_test\n",
    "\n",
    "\n",
    "################\n",
    "# Quality gate #\n",
    "################\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(features, labels)\n",
    "y_train, y_test = encode_labels(y_train, y_test)\n",
    "print(\"Label of first sample after OneHot encoding:\", y_train[0])\n",
    "quality_gate_141(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, Dropout, MaxPooling2D, GlobalMaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, 5,  input_shape = X_train[0].shape, activation = 'relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(8, 3, activation = 'relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(8, 3, activation = 'relu', padding=\"same\"))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(y_train[0].shape[0], activation = 'softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "optimizer=Adam(learning_rate=0.0005)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.array(X_train), np.array(y_train), epochs = 50, batch_size = 2, validation_split=0.1, \n",
    "          verbose = 1, sample_weight=compute_sample_weight('balanced', y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question:**\n",
    ">- How does the CNN perform compared to the ANN?\n",
    ">- What could be reasons for the different performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Your answer:** <span style=\"color:green\">TBD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Feedback:** <span style=\"color:orange\">...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task:**\n",
    ">With the above starter code, a first improvement in accuracy compared to the SVM and the ANN using only Dense layers should be visible. However, >the network could be further improved by adjusting the hyperparameters. Below you can find the full snippet from data preprocessing to model >training. Play around with the parameters and see whether you can find a model that shows an even better performance!\n",
    ">\n",
    ">Some ideas are:\n",
    ">- Explore different sized images (smaller/larger)\n",
    ">- How do black and white images compare to the rgb ones?\n",
    ">- Adapt the architecture of the neural network:\n",
    "> - Change the amount of Conv2D layers\n",
    "> - Change the number of filters in each layer\n",
    "> - Explore other activation functions\n",
    ">- Change the learning rate of the optimizer or look at different optimizers all together\n",
    ">- Train the model for more epochs\n",
    ">\n",
    ">**For comparability, please don't change the ratios for train/test and train/validation!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(28)\n",
    "\n",
    "####################################################\n",
    "# Please modify the following lines                #\n",
    "# ! Don't change training/test/validation ratios ! #\n",
    "####################################################\n",
    "\n",
    "# Data preprocessing\n",
    "features, labels = load_features_labels(\"./data/top\", size=(512,512), color=True, flatten=False, identifiers=['MC', 'MS', 'C'])\n",
    "features = np.array(features) # Datatype conversion of feature vector from list to array\n",
    "labels = np.array(labels) # Datatype conversion of label vector from list to array\n",
    "X_train, X_test, y_train, y_test = split_data(features, labels) # Split features and labels into training and testing datasets\n",
    "y_train, y_test = encode_labels(y_train, y_test) # Encode labels\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, 5,  input_shape = X_train[0].shape, activation = 'relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(8, 3, activation = 'relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(8, 3, activation = 'relu', padding=\"same\"))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(y_train[0].shape[0], activation = 'softmax'))\n",
    "\n",
    "# Model compilation\n",
    "optimizer=Adam(learning_rate=0.0005)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "# Model training\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs = 50, batch_size = 2, validation_split=0.1, \n",
    "          verbose = 1, sample_weight=compute_sample_weight('balanced', y_train))\n",
    "\n",
    "# Model evaluation\n",
    "evaluate_model(X_test, y_test, model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question:** Describe your approach optimizing the hyperparameters. Which behavior did you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.5: Data augmentation\n",
    "\n",
    "Data augmentation is a technique for artificially increasing the dataset without the need for additional data acquisition. The reason for this is, that most machine learning models perform better the higher the available data volume is. \n",
    "\n",
    "Data augmentation uses the principle of slight modifications to the original data to create new data, while using the labels of the existing image. As those modifications are rather small, the image as a whole is not changed by a lot and the to be identified object, or in our case image class, can still be recognized. However, the training process can be increased significantly. One can think of many variations of these slight modifications of an image. Typical examples include:\n",
    "- Random flipping of the image horizontally or vertically\n",
    "- Random rotations\n",
    "- Random shifts\n",
    "- Blurring the image\n",
    "- Adding artificially created noise\n",
    "- Cropping\n",
    "- Changes in contrast\n",
    "- Elastic deformations\n",
    "\n",
    "Below you can see some examples of the different augmentation strategies applied to our dataset\n",
    "\n",
    "#### Implementation in keras\n",
    "**Keras** includes its own procedure for image augmentation using the `ImageDataGenerator` generator. This generator offers a variety of data augmentation strategies, that are directly applied to the raw data during model training. Thus, the augmented data does not need to be stored to the disk.\n",
    "\n",
    "For this exercise, we are going to use the ImageDataGenerator from keras. Please have a look at the documentation to get familiar: https://keras.io/api/preprocessing/image/#imagedatagenerator-classData augmentation is a technique for artificially increasing the dataset without the need for additional data acquisition. The reason for this is, that most machine learning models perform better the higher the available data volume is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "features, labels = load_features_labels(\"./data/top\", size=(512,512), color=True, flatten=False, identifiers=['MC', 'MS', 'C'])\n",
    "features = np.array(features) # Datatype conversion of feature vector from list to array\n",
    "labels = np.array(labels) # Datatype conversion of label vector from list to array\n",
    "X_train, X_test, y_train, y_test = split_data(features, labels) # Split features and labels into training and testing datasets\n",
    "y_train, y_test = encode_labels(y_train, y_test) # Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create and show data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(features)) # Randomly select one image\n",
    "datagen.fit(features[[random_index]]) # Fit the image generator with the randomly selected image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the random augmentations\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(3, 4), axes_pad=(0.1, 0.3))\n",
    "\n",
    "grid[0].imshow(features[random_index])\n",
    "grid[0].set_title(\"Original\")\n",
    "for i, ax in enumerate(grid[1:]):\n",
    "    image = datagen.flow(features[[random_index]]).next()[0].astype(int)\n",
    "    ax.imshow(image) # Plot image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run model training with given data generator\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, train_size=0.9, stratify=y_train, random_state=21)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "datagen.fit(np.array(X_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, 5,  input_shape = X_train[0].shape, activation = 'relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(8, 5, activation = 'relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(16, 5, activation = 'relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(16, 3, activation = 'relu', padding=\"same\"))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(y_train[0].shape[0], activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(datagen.flow(np.array(X_train), np.array(y_train), batch_size=8), validation_data=(X_validation, y_validation),\n",
    "          steps_per_epoch=len(X_train) / 8, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question:** What behavior can be observed while training the model using the data augmentation? Did it improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Task & Question:** Experiment with the different data augmentation parameters, are all of them similar effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Your answer:** <span style=\"color:green\">TBD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Feedback:** <span style=\"color:orange\">...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Expanding the project scope\n",
    "So far, only the classes *Complete*, *Missing cover*, and *Missing screw* were investigated. Those defects are easy to observe in the top view.\n",
    "The remaining defect *Not screwed* is hardly visible in the top view. Thus, information from the side view could be used to detect this defect.\n",
    "\n",
    ">**Task:** In this last part of the exercise, you are tasked to expand the current quality monitoring solution to also detect not fully fastened screws. As mentioned above, it might be useful to investigate the side view images to achieve this purpose.\n",
    ">\n",
    ">You can approach this problem using any of the above mentioned solutions such as SVMs, ANNs or CNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# You can use as many cells as you want, but insert them before the final task at the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question:** Please describe your approach for expanding the project scope briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Your answer:** <span style=\"color:green\">TBD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Feedback:** <span style=\"color:orange\">...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question:** What was your final prediction results? What would you do to further improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Your answer:** <span style=\"color:green\">TBD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Feedback:** <span style=\"color:orange\">...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Question:** Which challenges did you encounter while solving the problem? How did you solve those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Your answer:** <span style=\"color:green\">TBD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Feedback:** <span style=\"color:orange\">...</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
